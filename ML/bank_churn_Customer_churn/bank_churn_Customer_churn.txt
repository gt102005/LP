Letâ€™s go through it step-by-step, explaining the code, logic, and expected outputs clearly ğŸ‘‡

ğŸ§© 1. Import Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf


Explanation:

pandas, numpy: Data loading and processing.

seaborn, matplotlib: Visualization.

tensorflow: For building the neural network model.

âœ… Output: None (imports only).

ğŸ“¥ 2. Load the Dataset
df = pd.read_csv('Churn_Modelling.csv')
df.head()


Explanation:

Loads the bank churn dataset.

df.head() displays the first 5 rows.

âœ… Output:
A preview like this ğŸ‘‡

RowNumber	CustomerId	Surname	CreditScore	Geography	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Exited
1	15634602	Hargrave	619	France	Female	42	2	0.00	1	1	1	101348.88	1
...	...	...	...	...	...	...	...	...	...	...	...	...	...
ğŸ” 3. Dataset Info
df.info()


Explanation:

Prints data types, non-null counts, and memory usage.

âœ… Output:

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 10000 entries, 0 to 9999
Data columns (total 14 columns):
...
dtypes: float64(3), int64(6), object(5)

ğŸ“Š 4. Visualize Target Distribution
plt.xlabel('Exited')
plt.ylabel('Count')
df['Exited'].value_counts().plot.bar()
plt.show()


Explanation:

Shows how many customers exited (1) vs stayed (0).

Usually, churners are fewer â†’ imbalanced dataset.

âœ… Output: A bar chart showing counts of 0 (non-churn) vs 1 (churn).

ğŸŒ 5. Check Geography Counts
df['Geography'].value_counts()


Explanation:

Displays count of customers from each country.

âœ… Output Example:

France     5014
Germany    2509
Spain      2477

ğŸ§± 6. One-Hot Encode Geography
df = pd.concat([df, pd.get_dummies(df['Geography'], prefix='Geo')], axis=1)


Explanation:

Converts the Geography column (France, Spain, Germany) into 3 numeric columns:

Geo_France, Geo_Spain, Geo_Germany

Helps the model use categorical data.

âœ… Output: None â€” DataFrame updated.

ğŸ‘©â€ğŸ¦° 7. One-Hot Encode Gender
df = pd.concat([df, pd.get_dummies(df['Gender'])], axis=1)


Explanation:

Adds two columns: Female and Male.

âœ… Output: None â€” new columns added.

ğŸ§¾ 8. Check Updated Dataset
df.info()


Explanation:

Now shows additional dummy variables and fewer categorical columns.

âœ… Output:

...
Geo_France       10000 non-null
Geo_Germany      10000 non-null
Geo_Spain        10000 non-null
Female           10000 non-null
Male             10000 non-null

ğŸ§¹ 9. Drop Unnecessary Columns
df.drop(columns=['RowNumber','CustomerId','Surname','Geography','Gender'], inplace=True)


Explanation:

Removes irrelevant or redundant columns that donâ€™t help prediction.

âœ… Output: None (in-place operation).

ğŸ‘€ 10. Preview Cleaned Data
df.head()


âœ… Output:
Data now contains only numeric columns â€” ready for model training.

ğŸ¯ 11. Split into Features (X) and Target (y)
y = df['Exited'].values
x = df.loc[:, df.columns != 'Exited'].values


Explanation:

y: target column (Exited) â€” 1 for churn, 0 for stay.

x: all other columns (independent features).

âœ… Output: None, but:

x.shape â†’ (10000, 13)

y.shape â†’ (10000,)

ğŸ§ª 12. Train-Test Split
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=20, test_size=0.25)


Explanation:

75% training, 25% testing split.

âœ… Output: None â€” creates 4 datasets.

âš–ï¸ 13. Normalize the Features
from sklearn.preprocessing import StandardScaler
std_x = StandardScaler()
x_train = std_x.fit_transform(x_train)
x_test = std_x.transform(x_test)


Explanation:

Standardizes the numeric features to mean = 0, std = 1.

Essential for neural network convergence.

âœ… Output: None â€” scaled arrays.

ğŸ“ 14. Check Shape
x_train.shape


âœ… Output:

(7500, 13)


13 features per sample.

ğŸ§  15. Define Neural Network (ANN)
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.models import Sequential

model = Sequential()
model.add(Flatten(input_shape=(13,)))
model.add(Dense(100, activation='relu'))
model.add(Dense(1, activation='sigmoid'))


Explanation:

Flatten: ensures input shape = 13.

Dense(100, relu): 1 hidden layer with 100 neurons.

Dense(1, sigmoid): output layer for binary classification (0/1).

âœ… Output: Model summary (if you call model.summary()).

âš™ï¸ 16. Compile Model
model.compile(optimizer='adam', metrics=['accuracy'], loss='BinaryCrossentropy')


Explanation:

optimizer='adam': efficient gradient descent.

loss='BinaryCrossentropy': suited for binary classification.

metrics=['accuracy']: tracks accuracy during training.

âœ… Output: None.

ğŸ‹ï¸ 17. Train the Model
model.fit(x_train, y_train, batch_size=64, validation_split=0.1, epochs=100)


Explanation:

Trains for 100 epochs with mini-batches of 64 samples.

10% of training data is used for validation.

âœ… Output:
Training logs per epoch:

Epoch 1/100
...
val_accuracy: 0.86


Youâ€™ll see accuracy improve and stabilize around 0.85â€“0.87.

ğŸ”® 18. Make Predictions
pred = model.predict(x_test)


Explanation:

Predicts probabilities (values between 0 and 1) for each test sample.

âœ… Output Example:

array([[0.032], [0.87], [0.45], ...])

ğŸ”¢ 19. Convert Probabilities to Classes
y_pred = []
for val in pred:
    if val > 0.5:
        y_pred.append(1)
    else:
        y_pred.append(0)


Explanation:

Converts predicted probabilities into binary labels:

0.5 â†’ churn (1)

â‰¤ 0.5 â†’ not churn (0)

âœ… Output: None â€” creates list of 0/1 predictions.

ğŸ“Š 20. Evaluate Model
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay


Then youâ€™d typically add:

acc = accuracy_score(y_test, y_pred)
print("Accuracy:", acc)
cm = confusion_matrix(y_test, y_pred)
ConfusionMatrixDisplay(cm).plot()


Explanation:

accuracy_score â†’ overall performance

confusion_matrix â†’ counts of TP, TN, FP, FN

ConfusionMatrixDisplay â†’ visualization

âœ… Output Example:

Accuracy: 0.86


Confusion matrix plot:

	Predicted 0	Predicted 1
Actual 0	1900	100
Actual 1	250	250
âœ… Final Summary
Step	Purpose	Output
1â€“3	Load & inspect dataset	Basic info
4â€“7	Visualize & encode categorical data	Dummy variables created
8â€“10	Drop unused cols	Clean numeric dataset
11â€“13	Split & scale data	Ready for model
15â€“17	Build & train ANN	Model learns to classify churn
18â€“20	Predict & evaluate	Accuracy ~85â€“87%, Confusion Matrix plot