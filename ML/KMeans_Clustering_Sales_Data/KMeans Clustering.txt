============================================================
                K-MEANS CLUSTERING EXPLANATION
============================================================

üìÅ FILE: KMeans.ipynb
üéØ PURPOSE: Customer or sales segmentation using K-Means clustering 
            on sales_data_sample.csv
============================================================


1Ô∏è‚É£ IMPORT LIBRARIES
------------------------------------------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

Explanation:
- pandas: For data manipulation and analysis.
- numpy: For mathematical operations.
- matplotlib & seaborn: For visualizing results.

Output: None.


2Ô∏è‚É£ LOAD THE DATASET
------------------------------------------------------------
df = pd.read_csv('sales_data_sample.csv', encoding='unicode_escape')
df.head()

Explanation:
- Loads the sales dataset.
- unicode_escape encoding fixes special character issues.
- Displays first 5 rows of the dataset.

Typical Columns:
------------------------------------------------------------
ORDERNUMBER | QUANTITYORDERED | PRICEEACH | SALES | STATUS | QTR_ID | 
MONTH_ID | YEAR_ID | PRODUCTLINE | DEALSIZE | COUNTRY | CUSTOMERNAME | ...

Output: First 5 rows printed.


3Ô∏è‚É£ VIEW DATA INFORMATION
------------------------------------------------------------
df.info()

Explanation:
Displays:
- Total number of entries
- Column names
- Data types
- Null value counts

Output Example:
------------------------------------------------------------
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2823 entries, 0 to 2822
Data columns (total 25 columns)
...


4Ô∏è‚É£ REMOVE UNNECESSARY COLUMNS
------------------------------------------------------------
df_drop  = [
 'ADDRESSLINE1','ADDRESSLINE2','POSTALCODE','CITY','TERRITORY',
 'PHONE','STATE','CONTACTFIRSTNAME','CONTACTLASTNAME','CUSTOMERNAME','ORDERNUMBER'
]
df = df.drop(df_drop, axis=1)

Explanation:
- Drops textual or identifier columns not needed for clustering.
- Removes redundant customer information and contact details.

Output: None.


5Ô∏è‚É£ VERIFY DATA AGAIN
------------------------------------------------------------
df.info()

Explanation:
Confirms that unnecessary columns were successfully removed.


6Ô∏è‚É£ CHECK VALUE DISTRIBUTIONS
------------------------------------------------------------
for col in df.columns.values:
    print(df[col].value_counts())

Explanation:
Prints frequency counts for each column.
Useful for identifying categorical columns and unique values.

Output Example:
------------------------------------------------------------
PRODUCTLINE
Classic Cars    900
Motorcycles     500
...
STATUS
Shipped         2000
Cancelled        100
...


7Ô∏è‚É£ REMOVE NON-NUMERIC OR NON-USEFUL COLUMNS
------------------------------------------------------------
df.drop(columns=['ORDERDATE','STATUS','MONTH_ID','QTR_ID','YEAR_ID'], inplace=True)
df.head()

Explanation:
- Removes date/time and status fields which are not directly useful 
  for clustering patterns.
- Keeps numeric and categorical columns relevant for segmentation.

Output: Head of cleaned DataFrame.


8Ô∏è‚É£ ENCODE CATEGORICAL VARIABLES
------------------------------------------------------------
from sklearn.preprocessing import LabelEncoder
def convert_categories(col):
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].values)

categories = ['PRODUCTLINE','PRODUCTCODE','COUNTRY','DEALSIZE']
for col in categories:
    convert_categories(col)

Explanation:
- Converts categorical (text) columns into numeric form using LabelEncoder.
- Example:
    PRODUCTLINE ‚Üí {Classic Cars:0, Motorcycles:1, Trucks and Buses:2, ...}
    COUNTRY ‚Üí {USA:0, France:1, Japan:2, ...}

Output: None, but data is now numeric and ready for scaling.


9Ô∏è‚É£ VIEW CLEANED DATA
------------------------------------------------------------
df.head()

Explanation:
Shows the dataset after encoding.
Now all columns are numeric, suitable for ML algorithms.


üîü STANDARDIZE THE DATA
------------------------------------------------------------
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
data = sc.fit_transform(df)

Explanation:
- Scales all features to have mean = 0 and standard deviation = 1.
- Essential for K-Means because it‚Äôs distance-based (Euclidean).

Output: None (transformed data stored in variable ‚Äòdata‚Äô).


1Ô∏è‚É£1Ô∏è‚É£ FIND OPTIMAL NUMBER OF CLUSTERS (ELBOW METHOD)
------------------------------------------------------------
from sklearn.cluster import KMeans
wcss = []
for k in range(1, 15):
    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=15)
    kmeans.fit(data)
    wcss.append(kmeans.inertia_)

Explanation:
- Runs K-Means for cluster counts from 1 to 14.
- inertia_ = Within-Cluster Sum of Squares (WCSS)
- Lower WCSS = tighter clusters.
- Used to determine ideal number of clusters.

Output: WCSS values stored in list.


1Ô∏è‚É£2Ô∏è‚É£ PLOT ELBOW CURVE
------------------------------------------------------------
k = list(range(1,15))
plt.plot(k, wcss)
plt.xlabel('Clusters')
plt.ylabel('Scores')
plt.title('Finding right number of clusters')
plt.grid()
plt.show()

Explanation:
- Plots number of clusters vs. WCSS (inertia).
- The "elbow point" where the curve bends sharply 
  indicates the best number of clusters (optimal K).

Output:
A line plot showing a curve decreasing with k.
The elbow typically appears around K=3 or K=4.


============================================================
üîç UNDERSTANDING K-MEANS CONCEPT
============================================================

| Term                    | Meaning / Purpose                                  |
|--------------------------|----------------------------------------------------|
| K-Means Algorithm        | Unsupervised ML algorithm for grouping data        |
| Cluster (K)              | Number of groups you want to find                 |
| Centroid                 | Center of each cluster                            |
| WCSS                     | Sum of squared distances within each cluster      |
| Elbow Method             | Finds best K where WCSS stops decreasing sharply  |
| StandardScaler           | Normalizes data for better distance comparison    |


============================================================
üßæ SUMMARY OF STEPS
============================================================

| Step | Section Name                    | Purpose                                  | Output                                  |
|------|----------------------------------|------------------------------------------|------------------------------------------|
| 1    | Import Libraries                 | Load Python ML & visualization tools     | None                                    |
| 2    | Load Dataset                     | Read sales data                          | First 5 rows                            |
| 3    | View Info                        | Check datatypes and nulls                | Data overview                           |
| 4    | Drop Unnecessary Columns         | Remove redundant data                    | Clean dataset                           |
| 5    | Check Again                      | Confirm removal                          | Info table                              |
| 6    | Value Counts                     | Inspect unique values                    | Frequency lists                         |
| 7    | Drop Non-Numeric Columns         | Remove irrelevant columns                | Clean numeric data                      |
| 8    | Encode Categorical Data          | Convert strings to numbers               | Numeric dataset                         |
| 9    | View Clean Data                  | Inspect final numeric form               | Head of df                              |
| 10   | Standardize Data                 | Normalize features                       | Scaled array                            |
| 11   | Fit KMeans (1‚Äì14 clusters)       | Compute WCSS for multiple Ks             | WCSS list                               |
| 12   | Plot Elbow Curve                 | Determine optimal cluster count          | Elbow plot                              |


============================================================
‚úÖ FINAL RESULT
============================================================

After plotting the Elbow Curve:
- Identify the elbow point (commonly around K=3 or K=4)
- Use that K value for clustering customers/products.

Interpretation:
Customers/products are grouped based on:
  - Purchase value
  - Deal size
  - Product line
  - Country
  - Quantity ordered
  - Sales amount

This helps understand customer behavior segments 
and sales performance across product categories.

============================================================
üß† KEY INSIGHTS
============================================================

- K-Means is distance-based, so scaling features is crucial.
- Label encoding converts text to numeric but assumes no order.
- Elbow point indicates the right cluster count visually.
- Clustering helps identify sales or customer segments 
  (e.g., high spenders, frequent buyers, etc.)

------------------------------------------------------------
üéØ Function minimized:
   K-Means minimizes intra-cluster variance (WCSS)
------------------------------------------------------------
